충돌 가능 영역 기반 시각 장애인 보행 충돌 위험 예측 모델
임석범, 이승재, 윤종우, 이규행*
단국대학교
{ andylim1022, jaelee1999, whddn1649, kyuhaeng.lee }@dankook.ac.kr
Collision Risk Zone-Based Collision Prediction Model
for Assisting Visually Impaired Pedestrians
Seokbeom Lim, Seungjae Lee, Jongwoo Yoon, Kyuhaeng Lee*
Dankook University


0. Abstract

 본 연구는 시각장애인의 보행 중 충돌 위험을 예측하기 위한 모델을 제안한다. 사람은 보행 중 무의식적으로 자신의 진행
방향에 해당하는 공간을 인지하고, 그 안에서 발생할 수 있는 위험 요소를 감지하려는 경향을 보인다. 본 연구에서는 이러한 인지적
직관을 수학적으로 모델링하여, 사용자의 체형과 카메라 시야각을 기반으로 '충돌 가능 영역'을 정의한다. 이 영역은 객체의 위치
정보, 영역과의 겹침 정도(IoU), 깊이 정보를 입력으로 활용한다. 충돌 가능 영역이 실제 충돌과 통계적으로 연관이 있는지를 자체
시뮬레이션 데이터를 통해 검증한 결과, IoU 단일 지표만으로도 AUROC 0.702의 분류 성능을 보여 그 유의성을 입증하였다. 이
러한 분석을 기반으로, YOLO와 MiDaS 기반 모델을 통해 위치와 깊이 정보를 추출해 시계열 입력으로 구성하였다. 위치·IoU·깊이
정보는 각각 Conv1D와 Conv3D를 통해 시공간적 특징을 추출한 후, Conv1D–BiLSTM으로 시간적 패턴을 학습하여 충돌 여부를
이진 분류한다. 실험 결과, IoU와 깊이 정보를 함께 활용한 최종 모델은 F1-score 0.7549를 기록하며 높은 예측 성능을 보였다.




1. Introduction

시각장애인의 독립적이고 안전한 보행은 시각장애인들의
삶의 질을 보장하기 위한 중요한 과제이다. 특히 보행 과정
에서 발생할 수 있는 장애물이나 이동체와의 충돌 위험은
생명과 직결되는 심각한 문제로, 이를 사전에 인식하고 예방
하는 기술적 대응이 요구된다. 최근 딥러닝 기반 객체 탐지
기술의 발전으로 다양한 환경에서의 객체 인식이 가능해졌지만, 대부분의 기존 보행 지원 시스템은 단일 프레임 기반의 객체
판단에 머무르고 있으며, 시계열적 변화나 공간적 맥락을 충분히
고려하지 못한다는 한계를 지닌다. 사람은 보행 중 무의식적으로 자신의
진행 방향을 기준으로 앞으로 지나갈 영역을 인지하고, 해당 공간
에서의 위험 요소를 회피하려는 경향을 보인다. 본 연구는 이러한
인지적 직관을 수학적으로 모델링 하여, 사용자를 기준으로 정의된
충돌 가능 영역 이라는 개념을 중심으로 실제 보행중의 위험을
정량적으로 예측하는 모델을 제안한다. 충돌 가능 영역의 개념
은 카메라 시야각과 사용자의 체형 정보를 바탕으로 수학적
으로 정의 하며, 본 연구에서 정의된 개념은 실제 보행 중 충돌과
통계적으로 유의미한 연관이 있음을 18만 개 이상의 시뮬레이션
시퀀스를 통해 입증하였다. 제안된 충돌 위험 예측 모델은 인식
된 객체와 충돌 가능 영역 간의 겹침정도(IoU)와 깊이 정보를
바탕으로 공간적, 시간적 특성을 추출하고, 이를 통합하여 충돌
가능성을 정량화 한다. 본 논문의 주요 공헌은 다음과 같다. 첫째, 시각장애인의 보행 중 충돌 위험을 정량화하기 위해
정의된 충돌 가능 영역의 유효성을 자체 수집 데이터를 기반
으로 통계적으로 검증하고, 이를 통해 해당 영역 정보가 예측
지표로서 유의미함을 입증하였다. 둘째, 검증된 영역 관련 지표를
바탕으로, IoU 및 깊이 정보를 통합하여 설계한 충돌 예측 모델은
F1-score 0.7549의 성능을 기록하며 높은 정확도를 달성하였다. 본 연구는 2024년 과학기술정통신부 및 정보통신기획평가원의 SW중심대학사업
지원을 받아 수행되었음
셋째, 충돌 여부를 시계열 분류 문제로 정의하고 이를 실험적으로 평가하여, 실제 환경에서 활용 가능한 충돌 예측 모델의
가능성을 제시하였다. 


2. Related Work

시각장애인 보행 지원을 위한 연구는 주로 객체 인식 기술을활용하여 장애물을 탐지하고, 객체 정보를 기반으로 경고나안내를
제공하는 방식에 초점을 맞추어왔다. 시각장애인 보행 보조를
위한 조도 적응형 실시간 객체 탐지[1]에서는 YOLO를 이용하여
경고를 주는 객체 탐지기법을 제안하였다. 또한, 딥러닝 기반
시각장애인 보행 보조 시스템[2] 연구에서는 YOLO 모델을이용하여 사람, 차량 등 주요 객체를 인식하고 음성 피드백을제공하는 시스템을 제안하였다. 객체 인식 기술을 넘어 시간적
연속성을 반영하려는 시도 또한 있었는데, Erdaw et al.[3]은
YOLO 모델에 Short-Term Memory (STM)를 결합하여 객체
이동 정보를 반영하는 탐지 시스템을 제안하였다. GloSea6에서
YOLOv8을 활용한 시각장애인 보행자 위험 감지[4] 연구에서는객체의 이동 방향, 크기 변화율과 깊이 추정을 통해 이미지의
위험도를 측정하였다. 기존의 선행 연구들은 객체 인식, 위험도 추정, 보행 환경 고려
등 다양한 측면에서 시각장애인의 보행 안전을 향상시키고자 하였다. 그러나 대부분의 기존 연구는 객체를 인식하는 연구에 초점을
맞추고 있으며, 탐지된 정보를 기반으로 정적인 위험 요소와
객체의 분류 만을 판단하는 방식이 주를 이루고 있다. 즉, 비전
기반으로 탐지된 객체의 시계열 움직임을 분석하고, 이를 통해
충돌 위험성을 수치화하는 방식은 아직 충분히 다뤄지지 않았다. 본 연구는 연속 프레임 간 정보를 시계열적으로 해석하여 충돌 가능성을 정량적으로 분석한다는 점에서 기존 연구와 차별성을 가진다.


3. Background

시각장애인 보행 지원을 위한 연구는 주로 객체 인식 기술을활용하여 장애물을 탐지하고, 객체 정보를 기반으로 경고나안내를
제공하는 방식에 초점을 맞추어왔다. 시각장애인 보행 보조를
위한 조도 적응형 실시간 객체 탐지[1]에서는 YOLO를 이용하여
경고를 주는 객체 탐지기법을 제안하였다. 또한, 딥러닝 기반
시각장애인 보행 보조 시스템[2] 연구에서는 YOLO 모델을이용하여 사람, 차량 등 주요 객체를 인식하고 음성 피드백을제공하는 시스템을 제안하였다. 객체 인식 기술을 넘어 시간적
연속성을 반영하려는 시도 또한 있었는데, Erdaw et al.[3]은
YOLO 모델에 Short-Term Memory (STM)를 결합하여 객체
이동 정보를 반영하는 탐지 시스템을 제안하였다. GloSea6에서
YOLOv8을 활용한 시각장애인 보행자 위험 감지[4] 연구에서는객체의 이동 방향, 크기 변화율과 깊이 추정을 통해 이미지의
위험도를 측정하였다. 기존의 선행 연구들은 객체 인식, 위험도 추정, 보행 환경 고려
등 다양한 측면에서 시각장애인의 보행 안전을 향상시키고자 하였다. 그러나 대부분의 기존 연구는 객체를 인식하는 연구에 초점을
맞추고 있으며, 탐지된 정보를 기반으로 정적인 위험 요소와
객체의 분류 만을 판단하는 방식이 주를 이루고 있다. 즉, 비전
기반으로 탐지된 객체의 시계열 움직임을 분석하고, 이를 통해
충돌 위험성을 수치화하는 방식은 아직 충분히 다뤄지지 않았다. 본 연구는 연속 프레임 간 정보를 시계열적으로 해석하여 충돌 가능성을 정량적으로 분석한다는 점에서 기존 연구와 차별성을 가진다.



4. Method

 본 논문은 시각 장애인의 보행중 발생 할 수 있는 충돌 위험을
예측하기 위해 객체 탐지, 깊이 추정, 시계열 정보 분석을 통합하는
모델을 설계한다. 모델은 객체의 시계열적 움직임과 공간적 특성을
반영하여 미래의 충돌 가능성을 정량적으로 판단한다. 이를
위해, 보행 환경에서의 위험 공간을 수학적으로 정의한 충돌 가능
영역을 주요 입력 특성으로 활용한다. 3.1 충돌가능 영역
보행 중 충돌 위험을 정량적으로 예측하기 위해, 본 연구는 카메라
시야를 기준으로 충돌 가능성이 높은 공간을 수학적으로 정의하였다. 충돌 가능 영역은 사용자의 시야 높이, 어깨 너비, 카메라의 수평 및
수직 시야각(VFOV, HFOV)을 기반으로 계산되며, 실제 보행 경로 상
에서 충돌 위험이 높은 영역을 시야 내에 삼각형 형태로 모델링한
것이다. 충돌 가능 영역을 수학적으로 정의하기 위해 다음과 같은 주요
변수를 사용한다. 사람의 시야 높이를 , 카메라의 수직시야각 (Vertical
Field of View) 을 
,수평시야각 (Horizontal Field of View) 을 
, 시야의
하단이 지면과 교차하는 수평거리 , 그리고 사람의 몸통 너비를 라 정의한다.
그림 1. 수직 시점에서 본 수직 시야각
 카메라 시야에서의 최하단과의 수평거리 는   cot   으로
계산된다. 이는 [그림1] 에서 사용자 기준으로 시야 높이 에서 시야의 하단
경계까지 이어지는 수직 시선과 지면이 이루는 삼각형을 기반으로 도출
된 식이다.
그림 2. 수평 시점에서 본 수직 시야각
앞서 구한 거리 를 통해 카메라의 수평 시야 하단 경계와 지면
의 교차 지점에서의 수평 폭 를   tan   로 계산 할
수 있다. 이는 [그림2]의 사람이 점유하는 실제 공간 와 를 통해
충돌 가능 영역의 비율을 tan  로 구할 수 있다. 앞서
정의한 식들을 바탕으로, 카메라의 시야에서 지평선의 중심점과
시야 하단의 수평 폭을 연결하여 설정한다. [그림3]은 충돌 가능
영역에 대한 카메라 시야와 실제 보행 환경을 도식화 한 것이다.
3.2 모델 구조
본 연구의 모델 구조는 객체 인식, 깊이 추정, 시간 순 입력
처리, 그리고 충돌 위험 예측의 네 단계로 구성된다. 전체 시스템의흐름은 [그림4]와 같다.
그림 4. 충돌 예측 모델 전체 구조
 첫 번째 단계에서는 영상 스트림으로부터 각 프레임 단위로
객체를 인식한다. 이를 위해 YOLO v11[5] 기반 객체 탐지 모델을
통하여, 사람, 차량, 장애물 등 다양한 객체를 검출하고, 위치 정보를
바운딩 박스 형태로 추출한다. 해당 객체의 좌표 정보와 충돌 가능
영역과의 IoU(Intersection over Union)를 계산하여, 위험도 분석의
입력 특성으로 활용한다.
 두 번째 단계에서는 이미지의 각 픽셀에 대한 상대적인 거리(깊이)를 예측하여, 위험도 판단의 정밀도를 향상시킨다. 이를
위해 MiDaS 기반의 단일 이미지 깊이 추정 모델을 사용하며, 프레임을 입력으로 받아 각 픽셀의 깊이를 예측한다. 이후, 충돌 가능 영역에 해당하는 픽셀들을 선택적으로 추출하여해당
깊이 정보를 활용한다.
 세 번째 단계에서는 객체와 깊이 정보를 시간 순서에 따라 분석하기 위해, 고정 길이의 슬라이딩 윈도우 큐(queue)를 사용한다. 이 큐에는 각 프레임에서 검출된 객체의 위치, 충돌 가능 영역과의 IoU, 그리고 해당 영역의 깊이 정보가 저장된다. 새로운 프레임이 들어오면 오래된 데이터는 제거되고 최신 정보가 추가되며, 항상 일정한 길이를
유지한다. 이를 통해 최근 시점의 연속적인 정보를 안정적으로 관리하며, 객체의 움직임과 위험상황의 변화를 효과적으로 파악할 수 있다.
 마지막 단계에서는, 큐에 저장된 시계열 데이터를 기반으로
향후 충돌 위험을 예측한다. 큐에 있는 모든 객체 위치, 충돌
가능 영역과의 IoU, 그리고 해당 영역의 깊이 정보가 하나의
시퀀스로 구성되어 입력된다. 먼저, 프레임별 깊이 정보는
Conv3D 계열의 신경망을 통해 처리되며, Conv3D와 MaxPooling
연산을 반복하여 시간축을 유지한 상태에서 공간 해상도만을
점진적으로 축소하는 방식으로 각 시점의 공간적 특성을 효과적으로 추출한다. 한편, IoU 및 바운딩 박스 좌표 등의 객체 관련
정보는 별도로 Conv1D 계층을 통해 시점별 구조적 특징을
그림 3. 사용자 시야의 충돌 가능 영역
추출하고, 앞서 얻은 깊이 기반 정보와 시점별로 결합되어 하나의
통합된 feature 시퀀스로 구성된다. 이 통합 시퀸스는 Conv1D 층을 통해
국소적(Local) 시간 패턴을 추출 하고, BiLSTM을 통해 객체의 움직임
과 충돌 경향성을 반영하는 시간적 종속성을 파악한다. 마지막으로, Fully
Connected Layer를 통해 미래 시점의 충돌 가능성을 수치화하여 나타낸다. 



5. Experiment

4. 충돌 예측을 위한 통계 기반 검증 및 실험 평가
4.1 데이터셋
본 연구에서는 모델의 학습과 평가를 위해 총 세 가지 데이터셋을
활용하였다. 첫 번째는 AI-Hub에서 제공하는 인도 보행 영상
Bounding Box 데이터셋으로, 약 35만 장의 이미지로 구성되어 있다. 다양한 보행 환경에서 수집된 이 데이터셋은 사람 차량, 장애물 등의
객체가 포함되어 있으며 YOLO 기반 객체 탐지 모델의 학습에 활용
되었다. 두 번째는 동일한 인도 보행 영상 데이터 셋의 깊이 추정
데이터셋으로, 약 17만 장 이미지에 각 픽셀별 깊이 레이블이 포함
되어 있다. 이 데이터는 MiDaS 기반의 깊이 추정 모델을 학습하는
데 사용되었다. 세 번째 데이터셋은 본 연구에서 직접 수집한 약 23
만 장의 충돌 시뮬레이션 영상 데이터셋이다. 실제 보행 환경을 가정
하여 촬영되었으며, 객체와의 충돌 또는 비충돌 장면이 프레임 단위
로 라벨링이 되어 있다. 충돌 장면 자체는 전체 보행 시간에 비해
빈도와 지속 시간이 짧지만, 본 연구는 미래 2~5초 이내의 충돌
여부를 예측하는 구조를 가지므로 현재 입력 시퀸스가 향후 충돌
을 유발할 경우의 충돌 라벨도 포함된다. 그 결과, 전체 시퀸스는 총
230475프레임으로 구성되어 있으며, 이중, 충돌 라벨이 부여된 프레임
은 66,608장 (약 28.9%), 비충돌은 163,867장 (약 71.1%) 이다. 이 데이터
셋은 연구의 핵심 실험인 충돌 가능성 예측 모델 학습 및 검증에 사용
되며, 특히 3.2절에서 제시한 충돌 예측 분류기에 대한 학습 데이터로
활용된다. 또한, 이 데이터는 충돌 가능 영역의 유효성을 통계적으로
검정하는데에도 사용되어, 영역 기반 입력 특성의 타당성을 정량적으로
분석하는 근거 자료로 기능한다. 4.2 충돌가능영역의 통계 검정
 충돌 가능 영역의 유효성을 검증하기 위해, 자체 수집한 충돌
시뮬레이션 데이터셋의 시퀀스별 IoU 평균값을 분석하였다. 각 시퀸스는
3.2절에서 설명한 고정 길이의 큐를 기준으로 구성되며, 각 시퀀스
내 모든 프레임의 IoU 평균을 비교 지표로 활용되었다. [그림5]는
충돌군과 비충돌군의 분포를 보여준다.
그림 5. 충돌군과 비충돌군 분포
충돌 시퀀스의 평균 IoU는 0.0336, 비충돌 시퀀스는 0.0203으로 약
65%의 차이를 보였으며, Mann-Whitney U검정 결과 (p < 
) 두 분포
간 유의한 차이가 확인되었다. 또한, IoU 하나의 특성만으로도
AUROC 0.702의 성능을 보여, 충돌 예측에 일정 수준의 판별력을
갖는 것으로 나타났다. 이를 통해 충돌 가능 영역과 관련된 입력들이 실제
충돌 위험과 통계적으로 연관됨을 확인할 수 있었다.
4.3 실험 및 결과
충돌 가능 영역이 실제 충돌과 통계적으로 유의한 연관성을가진다는 사실이 확인됨에 따라, 본 연구에서는 해당 정보를 다양한조합으로 입력하여 예측 성능을 비교 실험하였다. 실험에는 세 가지
모델을 구성하였으며, 첫 번째는 객체의 바운딩 박스 정보만을 활용한 기본 모델(Base), 두 번째는 인식된 객체와 충돌 가능 영역 간의 IoU
특성을 추가한 모델(IOU), 세 번째는 IoU에 더해 해당 영역의 깊이 정보까지 포함한 모델(Depth)이다. 이를 통해 충돌 가능 영역 정보가
예측 성능 향상에 기여하는지를 정량적으로 평가하였다. 표 1의 결과에서 바운딩 박스 정보만을 사용한 Base 모델은 F1-score 0.5091로 제한된 성능을 보였다. 반면, 충돌 가능 영역 기반 IoU 정보를 추가한 모델은 F1-score 0.6301로 성능이 향상되었고, 여기에 깊이 정보를 결합한 Depth 모델은 F1-score 0.7549로 가장 우수한 결과를 기록 하였다. 이는 공간적 위험 정보를 포함하는 입력이 충돌 예측 성능 향상에 실질적으로 기여함을 시사한다.

표 1. 테스트 데이터 셋에 대한 모델별 평가 지표
Model Accuracy Recall Precision F1-Score
Base 0.6932 0.5395 0.4820 0.5091
IOU 0.7924 0.6619 0.6010 0.6301
Depth 0.8455 0.7825 0.7288 0.7549



6. Discussion
 본 연구는 객체 인식, 충돌 가능 영역 정의, 깊이 추정, 그리고
시계열 정보를 통합하여 시각장애인의 보행 중 충돌 위험을 예측하는 모델을 제안하였다. 충돌 가능 영역과 해당 영역의 깊이 정보를 입력 특성으로 포함함으로써, 기존의 단순 위치 기반 예측보다
높은 정확도와 정밀도를 달성하였으며, 통계적 분석을 통해 해당
입력 특성들이 실제 충돌과 유의한 연관성을 가진다는 점도 입증하였다. 특히, 다양한 시간적‧공간적 정보를 효과적으로 결합하여
위험 상황을 사전에 인지하고 예측할 수 있는 구조를 처음 제시하였다는 점에서 실용적 가치가 높다. 또한, 향후 거리 (깊이) 추정이
가능한 센서나 스테레오 카메라 등의 보조 장비를 함께 활용할 경우, 예측 정확도를 더욱 높일 수 있을 것으로 기대된다. 


7. Conclusion

 본 연구는 객체 인식, 충돌 가능 영역 정의, 깊이 추정, 그리고
시계열 정보를 통합하여 시각장애인의 보행 중 충돌 위험을 예측하는 모델을 제안하였다. 충돌 가능 영역과 해당 영역의 깊이 정보를 입력 특성으로 포함함으로써, 기존의 단순 위치 기반 예측보다
높은 정확도와 정밀도를 달성하였으며, 통계적 분석을 통해 해당
입력 특성들이 실제 충돌과 유의한 연관성을 가진다는 점도 입증하였다. 특히, 다양한 시간적‧공간적 정보를 효과적으로 결합하여
위험 상황을 사전에 인지하고 예측할 수 있는 구조를 처음 제시하였다는 점에서 실용적 가치가 높다. 또한, 향후 거리 (깊이) 추정이
가능한 센서나 스테레오 카메라 등의 보조 장비를 함께 활용할 경우, 예측 정확도를 더욱 높일 수 있을 것으로 기대된다. 





8. 참고 문헌 reference
[1] 이찬우, 정예준, 조문규, 박범준, & 전태현. (2025). 시각장애인 보행보조를 위한 조도 적응형 실시간 객체 탐지. "한국통신학회".
[2] 이상훈. (2021). 딥러닝 기반 시각장애인 보행 보조 시스템. "KIPS 2021 춘계학술대회 논문집“.
[3] A Real-Time Obstacle Detection and Classification System for
Assisting Blind and Visually Impaired People Based on YOLO Model.
"2023 International Conference on Information and Communication
Technology for Development for Africa (ICT4DA)".
[4] 김남규, 박진성, 김성연, 박혜성, 정성욱. (2024). GloSea6에서 YOLOv8을 활용한 시각장애인 보행자 위험 감지 기법. "한국정보전자통신기술학회".
[5] RedmonJoseph (2016). You Only Look Once: Unified, Real-Time
Object Detection. "CVPR (IEEE Conference on Computer Vision and
Pattern Recognition)".