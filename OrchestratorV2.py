"""
Orchestrator V2 - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë²„ì „
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TreeLLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ with ì„¸ë°€í•œ íŒŒë¼ë¯¸í„° ì œì–´
"""

from pathlib import Path
import json
from datetime import datetime
import os
import time
from typing import Dict, Any, Optional, Generator
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import sys
sys.path.append(str(Path(__file__).parent.parent))

# ì„¤ì • ë° ëª¨ë“ˆ ì„í¬íŠ¸
from config import TreeLLMConfig, load_config
from module.build import BuildStep
from module.split import run as split_run
from module.fuse import TreeBuilder
from module.audit import AuditStep
from module.edit_pass1 import EditPass1
from module.global_check import GlobalCheck
from module.edit_pass2 import EditPass2


class OrchestratorV2:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì ìš©ëœ Orchestrator"""
    
    def __init__(self, config: TreeLLMConfig = None, preset: str = "balanced"):
        """
        Args:
            config: TreeLLMConfig ì¸ìŠ¤í„´ìŠ¤
            preset: ì‚¬ì „ ì •ì˜ëœ ì„¤ì • í”„ë¦¬ì…‹ ("fast", "balanced", "thorough", "research")
        """
        self.config = config or load_config(preset)
        self.config.validate()
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.base_dir = Path(self.config.result_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        self.metrics = {
            "start_time": None,
            "end_time": None,
            "step_durations": {},
            "api_calls": 0,
            "total_tokens": 0
        }
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_level = getattr(logging, self.config.log_level)
        logging.basicConfig(
            level=log_level,
            format='[%(asctime)s] %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.base_dir / "orchestrator.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log(self, message: str, level: str = "INFO"):
        """í†µí•© ë¡œê¹…"""
        getattr(self.logger, level.lower())(message)
    
    def _save_intermediate(self, step_name: str, content: Any, file_type: str = "json"):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        if not self.config.save_intermediate_results:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{step_name}_{timestamp}.{file_type}"
        filepath = self.base_dir / filename
        
        if file_type == "json":
            filepath.write_text(json.dumps(content, indent=2, ensure_ascii=False), encoding="utf-8")
        else:
            filepath.write_text(str(content), encoding="utf-8")
        
        self.log(f"Saved intermediate result: {filename}", "DEBUG")
    
    def _measure_time(self, func):
        """ë°ì½”ë ˆì´í„°: ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        def wrapper(*args, **kwargs):
            step_name = func.__name__.replace("_run_", "")
            start = time.time()
            result = func(*args, **kwargs)
            duration = time.time() - start
            self.metrics["step_durations"][step_name] = duration
            self.log(f"{step_name} completed in {duration:.2f}s")
            return result
        return wrapper
    
    def run(self, infile_text: str) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ ëª¨ë“œ"""
        self.metrics["start_time"] = datetime.now()
        self.log(f"Starting TreeLLM pipeline with preset: {self.config}")
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ë¡œë“œ
        raw_text = Path(infile_text).read_text(encoding="utf-8")
        
        # ê²°ê³¼ ë°ì´í„° ì´ˆê¸°í™”
        result_data = {
            "config": self.config.__dict__,
            "metrics": self.metrics,
            "steps": []
        }
        
        try:
            # 1. Split
            sections = self._run_split(raw_text)
            result_data["steps"].append({
                "step": 1,
                "name": "Split",
                "sections": list(sections.keys()),
                "content": sections
            })
            
            # 2. Build (ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜)
            build_result = self._run_build(raw_text)
            result_data["steps"].append({
                "step": 2,
                "name": "Build",
                "content": build_result
            })
            
            # 3. Fuse
            tree_result = self._run_fuse(build_result)
            result_data["steps"].append({
                "step": 3,
                "name": "Fuse",
                "content": json.loads(tree_result)
            })
            
            # 4. Audit
            audit_result = self._run_audit(sections, json.loads(tree_result))
            result_data["steps"].append({
                "step": 4,
                "name": "Audit",
                "content": audit_result
            })
            
            # 5. EditPass1
            edit1_result = self._run_edit1(sections, audit_result)
            result_data["steps"].append({
                "step": 5,
                "name": "EditPass1",
                "content": edit1_result
            })
            
            # 6. GlobalCheck
            global_check_result = self._run_global_check(edit1_result)
            result_data["steps"].append({
                "step": 6,
                "name": "GlobalCheck",
                "content": global_check_result
            })
            
            # 7. EditPass2
            final_result = self._run_edit2(edit1_result, global_check_result)
            result_data["steps"].append({
                "step": 7,
                "name": "EditPass2",
                "content": final_result
            })
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics["end_time"] = datetime.now()
            self.metrics["total_duration"] = (
                self.metrics["end_time"] - self.metrics["start_time"]
            ).total_seconds()
            
            result_data["final"] = final_result
            result_data["metrics"] = self.metrics
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            self._save_final_results(result_data)
            
            self.log("Pipeline completed successfully!")
            return result_data
            
        except Exception as e:
            self.log(f"Pipeline failed: {str(e)}", "ERROR")
            raise
    
    @_measure_time
    def _run_split(self, raw_text: str) -> Dict[str, str]:
        """ë¬¸ì„œ ë¶„í•  ì‹¤í–‰"""
        self.log("Running Split step...")
        
        # ì„¤ì • ì ìš©
        sections = split_run(raw_text)
        
        # ì„¤ì •ì— ë”°ë¥¸ í›„ì²˜ë¦¬
        if self.config.split.merge_short_sections:
            sections = self._merge_short_sections(sections)
        
        # ê¸¸ì´ ì œí•œ ì ìš©
        for key, content in sections.items():
            if len(content) < self.config.split.min_section_length:
                self.log(f"Warning: Section '{key}' is too short ({len(content)} chars)", "WARNING")
            elif len(content) > self.config.split.max_section_length:
                sections[key] = content[:self.config.split.max_section_length]
                self.log(f"Truncated section '{key}' to max length", "WARNING")
        
        self._save_intermediate("split", sections)
        return sections
    
    @_measure_time
    def _run_build(self, raw_text: str) -> str:
        """Build ë‹¨ê³„ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
        self.log("Running Build step...")
        
        # BuildStep ì´ˆê¸°í™” (ëª¨ë¸ íŒŒë¼ë¯¸í„° ì ìš©)
        build_step = BuildStep(model=self.config.model.model_name)
        
        # ê¸°ì¡´ call_gpt ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ì„¤ì • ì ìš©
        original_call_gpt = build_step.call_gpt
        
        def enhanced_call_gpt(prompt: str, prompt_type: str = None) -> str:
            params = self.config.get_prompt_params(prompt_type) if prompt_type else self.config.get_model_params()
            
            # API í˜¸ì¶œ ì¬ì‹œë„ ë¡œì§
            for attempt in range(self.config.build.retry_attempts):
                try:
                    response = build_step.client.chat.completions.create(
                        model=self.config.model.model_name,
                        messages=[{"role": "user", "content": prompt}],
                        **params,
                        timeout=self.config.build.timeout
                    )
                    self.metrics["api_calls"] += 1
                    return response.choices[0].message.content.strip()
                except Exception as e:
                    if attempt < self.config.build.retry_attempts - 1:
                        self.log(f"API call failed, retrying... ({attempt + 1}/{self.config.build.retry_attempts})", "WARNING")
                        time.sleep(self.config.build.retry_delay)
                    else:
                        raise
        
        build_step.call_gpt = enhanced_call_gpt
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        if self.config.build.parallel_processing:
            result = self._run_build_parallel(build_step, raw_text)
        else:
            result = build_step.run(raw_text)
        
        self._save_intermediate("build", result, "txt")
        return result
    
    def _run_build_parallel(self, build_step: BuildStep, raw_text: str) -> str:
        """Build ë‹¨ê³„ ë³‘ë ¬ ì²˜ë¦¬"""
        prompts = build_step.load_prompts()
        outputs = [""] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=self.config.build.max_workers) as executor:
            future_to_index = {}
            
            for i, (pid, tmpl) in enumerate(prompts):
                prompt = tmpl.replace("{INPUT}", raw_text)
                future = executor.submit(
                    build_step.call_gpt, 
                    prompt, 
                    pid.replace("_fill_prompt", "")
                )
                future_to_index[future] = (i, pid)
            
            for future in as_completed(future_to_index):
                index, pid = future_to_index[future]
                try:
                    result = future.result()
                    outputs[index] = f"### {pid}\n{result}"
                    self.log(f"Completed: {pid}")
                except Exception as e:
                    self.log(f"Failed: {pid} - {str(e)}", "ERROR")
                    outputs[index] = f"### {pid}\n[ERROR: {str(e)}]"
        
        return "\n\n".join(outputs)
    
    @_measure_time
    def _run_fuse(self, build_result: str) -> str:
        """Tree êµ¬ì¡° ìƒì„±"""
        self.log("Running Fuse step...")
        
        builder = TreeBuilder()
        
        # ì„¤ì • ì ìš©ì„ ìœ„í•œ í›„ì²˜ë¦¬ í›…
        if hasattr(builder, 'run'):
            original_run = builder.run
            
            def enhanced_run(text: str) -> str:
                result = original_run(text)
                
                # íŠ¸ë¦¬ ìµœì í™” ì ìš©
                if self.config.fuse.optimization_params["enable_pruning"]:
                    result = self._prune_tree(result)
                if self.config.fuse.optimization_params["enable_rebalancing"]:
                    result = self._rebalance_tree(result)
                
                return result
            
            builder.run = enhanced_run
        
        result = builder.run(build_result)
        self._save_intermediate("tree", json.loads(result))
        return result
    
    @_measure_time
    def _run_audit(self, sections: Dict[str, str], tree_dict: Dict) -> str:
        """ê°ì‚¬ ë‹¨ê³„ ì‹¤í–‰"""
        self.log("Running Audit step...")
        
        audit_step = AuditStep()
        
        # ì—„ê²©ë„ ì„¤ì • ì ìš©
        if hasattr(audit_step, 'strictness_level'):
            audit_step.strictness_level = self.config.audit.strictness_level
        
        result = audit_step.run(sections, tree_dict)
        
        # ì ìˆ˜ ê³„ì‚° ë° ê²€ì¦
        if self.config.audit.detailed_feedback:
            score = self._calculate_audit_score(result)
            if score < self.config.audit.score_threshold:
                self.log(f"Warning: Audit score ({score:.2f}) below threshold ({self.config.audit.score_threshold})", "WARNING")
        
        self._save_intermediate("audit", result, "txt")
        return result
    
    @_measure_time
    def _run_edit1(self, sections: Dict[str, str], audit_result: str) -> Dict:
        """ì²« ë²ˆì§¸ í¸ì§‘ ë‹¨ê³„"""
        self.log("Running EditPass1...")
        
        edit1_step = EditPass1()
        
        # í¸ì§‘ íŒŒë¼ë¯¸í„° ì ìš©
        if hasattr(edit1_step, 'params'):
            edit1_step.params = self.config.edit.edit1_params
        
        result = edit1_step.run(sections, audit_result)
        self._save_intermediate("edit1", result)
        return result
    
    @_measure_time
    def _run_global_check(self, edit1_result: Dict) -> str:
        """ì „ì—­ ì¼ê´€ì„± ê²€ì‚¬"""
        self.log("Running GlobalCheck...")
        
        global_check = GlobalCheck()
        
        # ê²€ì‚¬ í•­ëª© ì„¤ì •
        if hasattr(global_check, 'check_items'):
            global_check.check_items = self.config.global_check.check_items
        
        result = global_check.run(edit1_result)
        
        # ì¼ê´€ì„± ì ìˆ˜ í™•ì¸
        coherence_score = self._calculate_coherence_score(result)
        if coherence_score < self.config.global_check.coherence_threshold:
            self.log(f"Warning: Coherence score ({coherence_score:.2f}) below threshold", "WARNING")
        
        self._save_intermediate("global_check", result, "txt")
        return result
    
    @_measure_time
    def _run_edit2(self, edit1_result: Dict, global_check_result: str) -> str:
        """ìµœì¢… í¸ì§‘ ë‹¨ê³„"""
        self.log("Running EditPass2...")
        
        edit2_step = EditPass2()
        
        # ìµœì¢… í¸ì§‘ íŒŒë¼ë¯¸í„° ì ìš©
        if hasattr(edit2_step, 'params'):
            edit2_step.params = self.config.edit.edit2_params
        
        result = edit2_step.run(json.dumps(edit1_result, ensure_ascii=False), global_check_result)
        
        # ì¶œë ¥ í˜•ì‹ ì ìš©
        if self.config.output.output_format == "latex":
            result = self._convert_to_latex(result)
        elif self.config.output.output_format == "html":
            result = self._convert_to_html(result)
        
        self._save_intermediate("final", result, "txt")
        return result
    
    def run_stream(self, infile_text: str) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ëª¨ë“œ"""
        self.metrics["start_time"] = datetime.now()
        self.log(f"Starting TreeLLM pipeline (streaming) with preset: {self.config}")
        
        raw_text = Path(infile_text).read_text(encoding="utf-8")
        
        try:
            # 1. Split
            yield json.dumps({"step": 1, "name": "Split", "status": "starting"})
            sections = self._run_split(raw_text)
            yield json.dumps({
                "step": 1, 
                "name": "Split", 
                "status": "completed",
                "preview": {k: v[:200] + "..." for k, v in sections.items() if v}
            })
            
            # 2. Build
            yield json.dumps({"step": 2, "name": "Build", "status": "starting"})
            build_result = self._run_build(raw_text)
            yield json.dumps({
                "step": 2,
                "name": "Build",
                "status": "completed",
                "preview": build_result[:500] + "..."
            })
            
            # 3. Fuse
            yield json.dumps({"step": 3, "name": "Fuse", "status": "starting"})
            tree_result = self._run_fuse(build_result)
            yield json.dumps({
                "step": 3,
                "name": "Fuse",
                "status": "completed",
                "preview": tree_result[:500] + "..."
            })
            
            # 4. Audit
            yield json.dumps({"step": 4, "name": "Audit", "status": "starting"})
            audit_result = self._run_audit(sections, json.loads(tree_result))
            yield json.dumps({
                "step": 4,
                "name": "Audit",
                "status": "completed",
                "preview": audit_result[:500] + "..."
            })
            
            # 5. EditPass1
            yield json.dumps({"step": 5, "name": "EditPass1", "status": "starting"})
            edit1_result = self._run_edit1(sections, audit_result)
            yield json.dumps({
                "step": 5,
                "name": "EditPass1",
                "status": "completed",
                "preview": str(edit1_result)[:500] + "..."
            })
            
            # 6. GlobalCheck
            yield json.dumps({"step": 6, "name": "GlobalCheck", "status": "starting"})
            global_check_result = self._run_global_check(edit1_result)
            yield json.dumps({
                "step": 6,
                "name": "GlobalCheck",
                "status": "completed",
                "preview": global_check_result[:500] + "..."
            })
            
            # 7. EditPass2
            yield json.dumps({"step": 7, "name": "EditPass2", "status": "starting"})
            final_result = self._run_edit2(edit1_result, global_check_result)
            yield json.dumps({
                "step": 7,
                "name": "EditPass2",
                "status": "completed",
                "content": final_result
            })
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics["end_time"] = datetime.now()
            self.metrics["total_duration"] = (
                self.metrics["end_time"] - self.metrics["start_time"]
            ).total_seconds()
            
            yield json.dumps({
                "step": 8,
                "name": "Complete",
                "metrics": self.metrics
            })
            
        except Exception as e:
            self.log(f"Pipeline failed: {str(e)}", "ERROR")
            yield json.dumps({
                "error": str(e),
                "step": "failed"
            })
    
    # í—¬í¼ ë©”ì„œë“œë“¤
    def _merge_short_sections(self, sections: Dict[str, str]) -> Dict[str, str]:
        """ì§§ì€ ì„¹ì…˜ ë³‘í•©"""
        merged = {}
        buffer = ""
        buffer_key = ""
        
        for key, content in sections.items():
            if len(content) < self.config.split.min_section_length:
                if buffer_key:
                    buffer += "\n\n" + content
                else:
                    buffer = content
                    buffer_key = key
            else:
                if buffer:
                    merged[buffer_key] = buffer
                    buffer = ""
                    buffer_key = ""
                merged[key] = content
        
        if buffer:
            merged[buffer_key] = buffer
        
        return merged
    
    def _prune_tree(self, tree_json: str) -> str:
        """íŠ¸ë¦¬ ê°€ì§€ì¹˜ê¸°"""
        tree = json.loads(tree_json)
        
        def prune_node(node: Dict) -> Optional[Dict]:
            if "content" in node and len(node.get("content", "")) < self.config.fuse.min_node_content_length:
                return None
            
            if "children" in node:
                node["children"] = [prune_node(child) for child in node["children"]]
                node["children"] = [child for child in node["children"] if child]
            
            return node
        
        pruned = prune_node(tree)
        return json.dumps(pruned, ensure_ascii=False)
    
    def _rebalance_tree(self, tree_json: str) -> str:
        """íŠ¸ë¦¬ ì¬ê· í˜•"""
        tree = json.loads(tree_json)
        
        def get_depth(node: Dict) -> int:
            if "children" not in node or not node["children"]:
                return 1
            return 1 + max(get_depth(child) for child in node["children"])
        
        def rebalance_node(node: Dict, max_depth: int) -> Dict:
            current_depth = get_depth(node)
            
            if current_depth > max_depth:
                # ê¹Šì´ê°€ ë„ˆë¬´ ê¹Šìœ¼ë©´ ì¼ë¶€ ë…¸ë“œë¥¼ ë³‘í•©
                if "children" in node and len(node["children"]) > 1:
                    mid = len(node["children"]) // 2
                    left_children = node["children"][:mid]
                    right_children = node["children"][mid:]
                    
                    node["children"] = [
                        {"type": "merged", "children": left_children},
                        {"type": "merged", "children": right_children}
                    ]
            
            if "children" in node:
                node["children"] = [rebalance_node(child, max_depth) for child in node["children"]]
            
            return node
        
        rebalanced = rebalance_node(tree, self.config.fuse.max_tree_depth)
        return json.dumps(rebalanced, ensure_ascii=False)
    
    def _calculate_audit_score(self, audit_result: str) -> float:
        """ê°ì‚¬ ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚°
        positive_keywords = ["good", "strong", "clear", "valid", "sound"]
        negative_keywords = ["weak", "unclear", "missing", "insufficient", "poor"]
        
        text_lower = audit_result.lower()
        positive_count = sum(1 for keyword in positive_keywords if keyword in text_lower)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text_lower)
        
        if positive_count + negative_count == 0:
            return 0.5
        
        return positive_count / (positive_count + negative_count)
    
    def _calculate_coherence_score(self, global_check_result: str) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚°
        coherence_indicators = ["consistent", "aligned", "coherent", "unified", "integrated"]
        incoherence_indicators = ["inconsistent", "contradictory", "misaligned", "fragmented"]
        
        text_lower = global_check_result.lower()
        coherent_count = sum(1 for indicator in coherence_indicators if indicator in text_lower)
        incoherent_count = sum(1 for indicator in incoherence_indicators if indicator in text_lower)
        
        if coherent_count + incoherent_count == 0:
            return 0.7
        
        return coherent_count / (coherent_count + incoherent_count)
    
    def _convert_to_latex(self, text: str) -> str:
        """LaTeX í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        latex_header = r"""\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\begin{document}

"""
        latex_footer = r"""
\end{document}"""
        
        # ê°„ë‹¨í•œ ë³€í™˜ ê·œì¹™
        text = text.replace("#", "\\section{")
        text = text.replace("\n\n", "}\n\n")
        
        return latex_header + text + latex_footer
    
    def _convert_to_html(self, text: str) -> str:
        """HTML í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        html_header = """<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
h1, h2, h3 { color: #333; }
</style>
</head>
<body>
"""
        html_footer = """</body>
</html>"""
        
        # ê°„ë‹¨í•œ ë³€í™˜ ê·œì¹™
        lines = text.split("\n")
        html_lines = []
        
        for line in lines:
            if line.startswith("#"):
                level = len(line) - len(line.lstrip("#"))
                content = line.lstrip("#").strip()
                html_lines.append(f"<h{level}>{content}</h{level}>")
            elif line.strip():
                html_lines.append(f"<p>{line}</p>")
        
        return html_header + "\n".join(html_lines) + html_footer
    
    def _save_final_results(self, result_data: Dict[str, Any]):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ê²°ê³¼ ì €ì¥
        json_path = self.base_dir / f"result_{timestamp}.json"
        json_path.write_text(json.dumps(result_data, indent=2, ensure_ascii=False), encoding="utf-8")
        
        # ìµœì¢… ë…¼ë¬¸ ì €ì¥
        final_path = self.base_dir / f"final_paper_{timestamp}.{self.config.output.output_format}"
        final_path.write_text(result_data["final"], encoding="utf-8")
        
        # ìš”ì•½ ìƒì„± ë° ì €ì¥
        if self.config.output.generate_summary:
            summary = self._generate_summary(result_data["final"])
            summary_path = self.base_dir / f"summary_{timestamp}.txt"
            summary_path.write_text(summary, encoding="utf-8")
        
        self.log(f"Results saved: {json_path}, {final_path}")
    
    def _generate_summary(self, text: str) -> str:
        """ìš”ì•½ ìƒì„±"""
        # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©)
        lines = text.split("\n")
        summary_lines = []
        
        for line in lines[:20]:  # ì²˜ìŒ 20ì¤„ë§Œ
            if line.strip() and not line.startswith("#"):
                summary_lines.append(line)
                if len(" ".join(summary_lines).split()) > self.config.output.summary_length:
                    break
        
        return " ".join(summary_lines)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="TreeLLM Orchestrator V2")
    parser.add_argument("input", help="Input file path")
    parser.add_argument("--preset", choices=["fast", "balanced", "thorough", "research"], 
                       default="balanced", help="Configuration preset")
    parser.add_argument("--model", help="Override model name")
    parser.add_argument("--temperature", type=float, help="Override temperature")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--stream", action="store_true", help="Enable streaming mode")
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    overrides = {}
    if args.model:
        overrides["model"] = {"model_name": args.model}
    if args.temperature:
        overrides["model"] = overrides.get("model", {})
        overrides["model"]["temperature"] = args.temperature
    if args.debug:
        overrides["debug_mode"] = True
        overrides["log_level"] = "DEBUG"
    
    config = load_config(args.preset, **overrides)
    
    # Orchestrator ì‹¤í–‰
    orchestrator = OrchestratorV2(config)
    
    if args.stream:
        print("Starting streaming mode...")
        for update in orchestrator.run_stream(args.input):
            print(update)
    else:
        result = orchestrator.run(args.input)
        print(f"\nâœ… Pipeline completed in {result['metrics']['total_duration']:.2f}s")
        print(f"ğŸ“Š API calls: {result['metrics']['api_calls']}")
        print(f"ğŸ“ Results saved to: {orchestrator.base_dir}")
