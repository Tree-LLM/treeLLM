# config.py - ì„¤ì • ê´€ë¦¬

import os
from typing import Optional
from dataclasses import dataclass

@dataclass
class TreeLLMConfig:
    """TreeLLM ì‹œìŠ¤í…œ ì„¤ì •"""
    
    # LLM ì„¤ì •
    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    default_llm_provider: str = "openai"
    default_model: str = "gpt-4"
    max_tokens_per_request: int = 2000
    temperature: float = 0.1
    enable_mock_mode: bool = False
    
    # íŒŒì¼ ì²˜ë¦¬ ì„¤ì •
    max_pdf_size_mb: int = 10
    temp_file_cleanup: bool = True
    max_related_papers: int = 10
    
    # ì›¹ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    streamlit_server_port: int = 8501
    streamlit_server_address: str = "localhost"
    
    # ë¡œê¹… ì„¤ì •
    log_level: str = "INFO"
    log_file: str = "logs/treellm.log"
    enable_agent_logging: bool = True
    
    def __post_init__(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY", self.openai_api_key)
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY", self.anthropic_api_key)
        self.default_llm_provider = os.getenv("DEFAULT_LLM_PROVIDER", self.default_llm_provider)
        self.default_model = os.getenv("DEFAULT_MODEL", self.default_model)
        
        # ìˆ«ì íƒ€ì… í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬
        self.max_tokens_per_request = int(os.getenv("MAX_TOKENS_PER_REQUEST", self.max_tokens_per_request))
        self.temperature = float(os.getenv("TEMPERATURE", self.temperature))
        self.max_pdf_size_mb = int(os.getenv("MAX_PDF_SIZE_MB", self.max_pdf_size_mb))
        self.max_related_papers = int(os.getenv("MAX_RELATED_PAPERS", self.max_related_papers))
        self.streamlit_server_port = int(os.getenv("STREAMLIT_SERVER_PORT", self.streamlit_server_port))
        
        # ë¶ˆë¦° íƒ€ì… í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬
        self.enable_mock_mode = os.getenv("ENABLE_MOCK_MODE", "false").lower() == "true"
        self.temp_file_cleanup = os.getenv("TEMP_FILE_CLEANUP", "true").lower() == "true"
        self.enable_agent_logging = os.getenv("ENABLE_AGENT_LOGGING", "true").lower() == "true"
        
        # ë¬¸ìì—´ íƒ€ì… í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬
        self.streamlit_server_address = os.getenv("STREAMLIT_SERVER_ADDRESS", self.streamlit_server_address)
        self.log_level = os.getenv("LOG_LEVEL", self.log_level)
        self.log_file = os.getenv("LOG_FILE", self.log_file)
    
    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        if not self.enable_mock_mode:
            if not self.openai_api_key and not self.anthropic_api_key:
                print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OPENAI_API_KEY ë˜ëŠ” ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                return False
        
        if self.default_llm_provider not in ["openai", "anthropic"]:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.default_llm_provider}")
            return False
        
        if self.max_tokens_per_request <= 0:
            print(f"âŒ ì˜ëª»ëœ max_tokens_per_request ê°’: {self.max_tokens_per_request}")
            return False
        
        return True
    
    def print_config(self):
        """í˜„ì¬ ì„¤ì • ì¶œë ¥"""
        print("ğŸ”§ TreeLLM ì„¤ì • ì •ë³´")
        print("=" * 30)
        print(f"LLM ì œê³µì: {self.default_llm_provider}")
        print(f"ëª¨ë¸: {self.default_model}")
        print(f"ìµœëŒ€ í† í°: {self.max_tokens_per_request:,}")
        print(f"Temperature: {self.temperature}")
        print(f"Mock ëª¨ë“œ: {'âœ…' if self.enable_mock_mode else 'âŒ'}")
        print(f"ìµœëŒ€ PDF í¬ê¸°: {self.max_pdf_size_mb}MB")
        print(f"ìµœëŒ€ ê´€ë ¨ ë…¼ë¬¸: {self.max_related_papers}í¸")
        print(f"ë¡œê·¸ ë ˆë²¨: {self.log_level}")
        
        # API í‚¤ ìƒíƒœ (ë³´ì•ˆìƒ ì¼ë¶€ë§Œ í‘œì‹œ)
        if self.openai_api_key:
            masked_key = self.openai_api_key[:8] + "..." + self.openai_api_key[-4:]
            print(f"OpenAI API: {masked_key}")
        
        if self.anthropic_api_key:
            masked_key = self.anthropic_api_key[:8] + "..." + self.anthropic_api_key[-4:]
            print(f"Anthropic API: {masked_key}")

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
config = TreeLLMConfig()

def load_config() -> TreeLLMConfig:
    """ì„¤ì • ë¡œë“œ ë° ê²€ì¦"""
    global config
    
    if not config.validate():
        print("\nğŸ’¡ ì„¤ì • í•´ê²° ë°©ë²•:")
        print("1. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        print("2. ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•˜ì„¸ìš”:")
        print("   export OPENAI_API_KEY='your-api-key'")
        print("3. í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œëŠ” ENABLE_MOCK_MODE=trueë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        
        return None
    
    return config

if __name__ == "__main__":
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    config = load_config()
    if config:
        config.print_config()
    else:
        print("âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨")
